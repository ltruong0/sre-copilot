# LLM Provider Configuration
# Options: ollama, watsonx
LLM_PROVIDER=ollama

# Ollama Configuration
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=granite4
OLLAMA_EMBEDDING_MODEL=granite-embedding:278m
# Model for cleaning badly formatted sections (smaller/faster)
OLLAMA_CLEANUP_MODEL=granite4:7b
# CA certificate for self-signed SSL (e.g., homelab Traefik)
# OLLAMA_CA_CERT=/path/to/rootCA.pem

# IBM watsonx.ai Configuration (optional - for cloud deployment)
# WATSONX_API_KEY=your-api-key
# WATSONX_PROJECT_ID=your-project-id
# WATSONX_URL=https://us-south.ml.cloud.ibm.com
# WATSONX_MODEL=ibm/granite-13b-chat-v2
# WATSONX_EMBEDDING_MODEL=ibm/slate-125m-english-rtrvr

# Document Ingestion
DOCS_PATH=./docs
CHROMADB_PATH=./data/chromadb

# Chunking Configuration
CHUNK_MIN_TOKENS=100
CHUNK_MAX_TOKENS=1000
CHUNK_TARGET_TOKENS=500

# RAG Configuration
RAG_TOP_K=5
RAG_SIMILARITY_THRESHOLD=0.3

# MCP Server Configuration
MCP_SERVER_HOST=127.0.0.1
MCP_SERVER_PORT=8080

# Logging
LOG_LEVEL=INFO
LOG_FORMAT=console
